{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e2fd46-f3e0-434e-a3e7-ac9986a61b4f",
   "metadata": {},
   "source": [
    "# 1.Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3de9868-2420-4820-939b-8263cb3bfbc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import lightning as L\n",
    "except:\n",
    "    import lightning as L\n",
    "\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from torchmetrics.classification import Accuracy\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from typing import Any, Callable, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "\n",
    "from torchvision.ops import StochasticDepth, MLP, Permute\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    RandAugment,\n",
    "    ToTensor,\n",
    "    Resize,\n",
    "    Lambda,\n",
    "    Normalize,\n",
    "    RandomRotation,\n",
    "    RandomHorizontalFlip,\n",
    "    CenterCrop,\n",
    "    RandomAdjustSharpness\n",
    ")\n",
    "from torchvision.transforms.v2 import RandomChoice\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.datasets.utils import (\n",
    "    download_url,\n",
    "    download_and_extract_archive\n",
    ")\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import loralib\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.facecolor'] = 'lightgray'\n",
    "plt.rcParams['mathtext.fontset'] = 'cm'\n",
    "plt.rcParams['font.family'] = 'STIXGeneral'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399e54cc-e121-453b-a2a2-cfb1cc50b275",
   "metadata": {},
   "source": [
    "# 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a427e36a-662a-47d1-81e9-864dce7d4c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATCH_SIZE            = [4, 4]\n",
    "EMBED_DIM             = 96\n",
    "DEPTHS                = [2, 2, 6, 2]\n",
    "NUM_HEADS             = [3, 6, 12, 24]\n",
    "WINDOW_SIZE           = [7, 7]\n",
    "STOCHASTIC_DEPTH_PROB = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "227c6629-4c10-47a6-a901-07c0c20ea897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE  = 256\n",
    "NUM_CLASSES = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a35f488c-40ef-4d46-a26d-f766bac5481f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCH      = 3 ** 2\n",
    "BATCH_SIZE = 6 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d169c3ee-81ff-44bb-825c-88f34879f13b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRUNCATE_PER_CATEGORY = int(1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ae120c7-621b-4e91-9e96-62a046505a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MOMENTUM = math.sqrt(6) / math.e\n",
    "GOLDEN_RATIO = (1. + math.sqrt(5)) / 2.\n",
    "EARLY_STOPPING_PATIENCE = 1 / 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12d4fb98-9308-467c-9f20-68a308155c44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WEIGHT_DECAY  = GOLDEN_RATIO  * 10 ** -math.pi\n",
    "LEARNING_RATE = GOLDEN_RATIO  * 10 ** -math.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d12f4e8-b2eb-4080-8065-c0bce9852a39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "METRIC_TO_MONITOR = \"val_acc\"\n",
    "METRIC_MODE       = \"max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07ff4a6f-2253-41af-b407-d65a38e86e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ACC_HISTORY     = dict()\n",
    "LOSS_HISTORY    = dict()\n",
    "MODEL_NAME      = dict()\n",
    "MODEL           = dict()\n",
    "BEST_MODEL_PATH = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa3eefc6-861b-41f0-88d7-32a2bb173a55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"experiment\", exist_ok=True)\n",
    "os.makedirs(\"experiment/training\", exist_ok=True)\n",
    "os.makedirs(\"experiment/dataset\", exist_ok=True)\n",
    "os.makedirs(\"experiment/model\", exist_ok=True)\n",
    "EXPERIMENT_DIR = \"experiment/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1fc5618-2a70-4a6c-b476-44f049970c23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 1478610483\n"
     ]
    }
   ],
   "source": [
    "SEED = int(np.random.randint(2147483647))\n",
    "print(f\"Random seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2206365f-ff87-455e-b32f-e603f63830d8",
   "metadata": {},
   "source": [
    "# 3. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed15b9-6eb8-43be-bc6c-0a0a37412768",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad447e0f-ee13-4e85-871d-6d2f88b52886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUG_TRANSFORM = Compose(\n",
    "    [\n",
    "        RandAugment(),\n",
    "        \n",
    "        RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "        RandomHorizontalFlip(p=0.5),\n",
    "        RandomRotation(35),\n",
    "        \n",
    "        Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        ToTensor(),\n",
    "        Lambda(lambda x: (x * 2) - 1),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "           \n",
    "    ]\n",
    ")\n",
    "\n",
    "TRANSFORM = Compose(\n",
    "    [\n",
    "        Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        ToTensor(),\n",
    "        Lambda(lambda x: (x * 2) - 1),\n",
    "        \n",
    "        Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f94481-bbaf-47dc-a694-1a810c253951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Caltech256(VisionDataset):\n",
    "    \"\"\"`Caltech 256 `_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory\n",
    "            ``ISIC2019`` exists or will be saved to if download is set to\n",
    "            True.\n",
    "        split (string): dataset split\n",
    "        transform (callable, optional): A function/transform that takes in a\n",
    "            PIL image and returns a transformed version.\n",
    "            E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes\n",
    "            in the target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the\n",
    "            internet and puts it in root directory. If dataset is already\n",
    "            downloaded, it is not downloaded again.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        split: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        download: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            os.path.join(root, \"caltech256\"),\n",
    "            transform=transform,\n",
    "            target_transform=target_transform,\n",
    "        )\n",
    "        os.makedirs(self.root, exist_ok=True)\n",
    "\n",
    "        assert split in [\"train\", \"val\", \"test\", \"inference\"], (\n",
    "            \"Please choose one of these: 'train', 'val', 'test', or 'inference'\"\n",
    "        )\n",
    "\n",
    "        if split == \"inference\":\n",
    "            assert self.transform is None and self.target_transform is None\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError(\n",
    "                \"Dataset not found or corrupted. You can use download=True\"\n",
    "                \"to download it\"\n",
    "            )\n",
    "\n",
    "        self.categories = sorted(\n",
    "            os.listdir(os.path.join(self.root, \"256_ObjectCategories\"))\n",
    "        )\n",
    "\n",
    "        self.y: List[int] = list()\n",
    "        self.x = list()\n",
    "        for i, c in enumerate(self.categories):\n",
    "            if i == NUM_CLASSES: break\n",
    "\n",
    "            image_path = [\n",
    "                os.path.join(\n",
    "                    self.root,\n",
    "                    \"256_ObjectCategories\",\n",
    "                    c,\n",
    "                    item,\n",
    "                )\n",
    "                for item in os.listdir(\n",
    "                    os.path.join(self.root, \"256_ObjectCategories\", c)\n",
    "                )\n",
    "                if item.endswith(\".jpg\")\n",
    "            ]\n",
    "            image_path = image_path[:TRUNCATE_PER_CATEGORY]\n",
    "\n",
    "            start = 0\n",
    "            end = 0\n",
    "\n",
    "            if self.split == \"train\":\n",
    "                end = int(0.81 * len(image_path))\n",
    "            elif self.split == \"val\":\n",
    "                start = int(0.81 * len(image_path))\n",
    "                end = int(0.9 * len(image_path))\n",
    "            else:\n",
    "                start = int(0.9 * len(image_path))\n",
    "                end = len(image_path)\n",
    "\n",
    "            image_path = image_path[start:end]\n",
    "\n",
    "            self.x.extend(image_path)\n",
    "            self.y.extend(len(image_path) * [i])\n",
    "\n",
    "        self.categories = [\n",
    "            cat.split(\".\")[-1].replace(\"-101\", \"\")\n",
    "            for idx, cat in enumerate(self.categories)\n",
    "            if idx < NUM_CLASSES\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img = Image.open(self.x[index]).convert('RGB')\n",
    "\n",
    "        target = self.y[index]\n",
    "\n",
    "        if self.split != \"inference\":\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def _check_integrity(self) -> bool:\n",
    "        # can be more robust and check hash of files\n",
    "        return os.path.exists(os.path.join(self.root, \"256_ObjectCategories\"))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # return len(self.index)\n",
    "        return len(self.y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
