{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f27c10f-4fd8-47cf-bebf-3963607d296d",
   "metadata": {},
   "source": [
    "# Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5e9ff8-373d-42e8-8e03-d0fa45767781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as f\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "660fa8e9-4826-498f-8a53-e8bb0972b888",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97128520-9ce7-4f6c-b06b-7fdbff17396c",
   "metadata": {},
   "source": [
    "# Swin Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74a45370-b252-447f-8ff7-ec2e70cf5b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fn = fn\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) +x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d5ea2ac-7837-4394-be64-a3629efaa3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        # return self.fn(self.norm(x), **kwargs) # swin T v1\n",
    "        return self.norm(self.fn(x), **kwargs) # swin T v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bb6f714-c2d0-4a75-8410-37067e15d89b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[[[ 154.0996,  -29.3429, -217.8789],\n",
      "          [  56.8431, -108.4522, -139.8595]],\n",
      "\n",
      "         [[  40.3347,   83.8026,  -71.9258],\n",
      "          [ -40.3344,  -59.6635,   18.2036]]]])\n",
      "output:  tensor([[[[ 1.2191,  0.0112, -1.2303],\n",
      "          [ 1.3985, -0.5173, -0.8813]],\n",
      "\n",
      "         [[ 0.3495,  1.0120, -1.3615],\n",
      "          [-0.3948, -0.9787,  1.3735]]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "B,H,W,C=1,2,2,3\n",
    "input = torch.randn(B,H,W,C)*100\n",
    "print(\"input: \", input)\n",
    "layer_norm = nn.LayerNorm(C)\n",
    "output = layer_norm(input)\n",
    "print(\"output: \",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c1697c8-fe27-418b-9491-5d0df836d16a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9, 7, 8],\n",
       "        [3, 1, 2],\n",
       "        [6, 4, 5]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).view(3, 3)\n",
    "print(x)\n",
    "torch.roll(x, shifts=(1, 1), dims=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0c9fd8d-4c4e-4359-9ff2-0f245dd7ae0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14., 15., 16., 17., 18.],\n",
      "        [19., 20., 21., 22., 23., 24., 25., 26., 27.],\n",
      "        [28., 29., 30., 31., 32., 33., 34., 35., 36.],\n",
      "        [37., 38., 39., 40., 41., 42., 43., 44., 45.],\n",
      "        [46., 47., 48., 49., 50., 51., 52., 53., 54.],\n",
      "        [55., 56., 57., 58., 59., 60., 61., 62., 63.],\n",
      "        [64., 65., 66., 67., 68., 69., 70., 71., 72.],\n",
      "        [73., 74., 75., 76., 77., 78., 79., 80., 81.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[11., 12., 13., 14., 15., 16., 17., 18., 10.],\n",
       "        [20., 21., 22., 23., 24., 25., 26., 27., 19.],\n",
       "        [29., 30., 31., 32., 33., 34., 35., 36., 28.],\n",
       "        [38., 39., 40., 41., 42., 43., 44., 45., 37.],\n",
       "        [47., 48., 49., 50., 51., 52., 53., 54., 46.],\n",
       "        [56., 57., 58., 59., 60., 61., 62., 63., 55.],\n",
       "        [65., 66., 67., 68., 69., 70., 71., 72., 64.],\n",
       "        [74., 75., 76., 77., 78., 79., 80., 81., 73.],\n",
       "        [ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.,  1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.linspace(1,81,81).view(9,9)\n",
    "print(x)\n",
    "torch.roll(x, shifts=(-1, -1), dims=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c22118ac-a26a-4ea3-a828-155e70f9cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, displacement):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.displacement = displacement\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d327277b-0715-463b-bbd5-3cebdd8c3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60ed8a07-d46c-440f-8843-1b463ce057e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mask(window_size, displacement, upper_lower, left_right):\n",
    "    mask = torch.zeros(window_size ** 2, window_size ** 2) # (49,49)\n",
    "    # print(\"Original mask: \", mask)\n",
    "    \n",
    "    if upper_lower:\n",
    "#          down lef section\n",
    "        mask[-displacement * window_size:, :-displacement*window_size] = float('-inf')\n",
    "#         up right section\n",
    "        mask[:-displacement * window_size, -displacement*window_size:] = float('-inf')\n",
    "    \n",
    "    if left_right:\n",
    "                                                                # to handle last vertical patches\n",
    "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size,h2=window_size)\n",
    "        \n",
    "        mask[:, -displacement:, :, :-displacement] = float('inf')\n",
    "        mask[:, :-displacement, :, -displacement:] = float('inf')\n",
    "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
    "        \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6243ac84-b869-4ebf-9ea8-37de243482a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., inf, 0., 0., inf, 0., 0., inf],\n",
       "        [0., 0., inf, 0., 0., inf, 0., 0., inf],\n",
       "        [inf, inf, 0., inf, inf, 0., inf, inf, 0.],\n",
       "        [0., 0., inf, 0., 0., inf, 0., 0., inf],\n",
       "        [0., 0., inf, 0., 0., inf, 0., 0., inf],\n",
       "        [inf, inf, 0., inf, inf, 0., inf, inf, 0.],\n",
       "        [0., 0., inf, 0., 0., inf, 0., 0., inf],\n",
       "        [0., 0., inf, 0., 0., inf, 0., 0., inf],\n",
       "        [inf, inf, 0., inf, inf, 0., inf, inf, 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_mask(window_size=3, displacement=1, upper_lower=False, left_right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42c7122e-a3af-4145-81a7-afedf7113067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_relative_distances(window_size):\n",
    "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
    "    distances = indices[None, :, :] - indices[:, None, :]\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "285940fa-06f3-4d59-bd39-6eccea4459a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        inner_dim = head_dim * heads\n",
    "        self.heads = heads\n",
    "        self.scale = head_dim ** -0.5          # scaling dot product inside softmax\n",
    "        self.window_size = window_size\n",
    "        self.relative_pos_embedding = relative_pos_embedding\n",
    "        self.shifted = shifted\n",
    "        \n",
    "        self.tau = nn.Parameter(torch.tensor(0.01), requires_grad=True)\n",
    "        \n",
    "        if self.shifted:\n",
    "            displacement = window_size // 2\n",
    "            self.cyclic_shift = CyclicShift(-displacement)\n",
    "            self.cyclic_back_shift = CyclicShift(displacement)\n",
    "            \n",
    "            \n",
    "            # (49,49): masks are NOT learnable parameters; requires_grad=False\n",
    "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size,\n",
    "                                                            displacement=displacement,\n",
    "                                                            upper_lower=True, left_right=False), \n",
    "                                                 requires_grad=False)\n",
    "            \n",
    "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size,\n",
    "                                                            displacement=displacement,\n",
    "                                                            upper_lower=False, left_right=True), \n",
    "                                                 requires_grad=False)\n",
    "            \n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        # dim = (96, 192, 384,384) and inner_dim=head_dim * heads, can also use C*3\n",
    "        \n",
    "        # self.pos_embedding= nn.Parameter(torch.randn(window_size**2, window_size**2)) #(49, 49)\n",
    "        \n",
    "        if self.relative_pos_embedding:\n",
    "            self.relative_indicies = get_relative_distances(window_size) + window_size - 1\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(2*window_size -1, 2*window_size -1))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
    "        \n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "        # inner_dim = head_dim * heads = C, dim=hidden_dim=(96,192,384,768)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.shifted:\n",
    "            x = self.cyclic_shift(x)\n",
    "            \n",
    "        b, n_h, n_w, _, h = *x.shape, self.heads\n",
    "        \n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        \n",
    "        nw_h = n_h // self.window_size\n",
    "        nw_w = n_w // self.window_size\n",
    "        \n",
    "        q,k,v = map(\n",
    "            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
    "            h = h, w_h = self.window_size, w_w = self.window_size), qkv\n",
    "        )\n",
    "        \n",
    "        # dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
    "        \n",
    "        q = f.normalize(q, p=2, dim=-1)\n",
    "        k = f.normalize(k, p=2, dim=-1)\n",
    "        \n",
    "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) / self.tau\n",
    "        \n",
    "        \n",
    "        if self.relative_pos_embedding:\n",
    "            dots += self.pos_embedding[self.relative_indicies[:,:,0], self.relative_indicies[:,:,1]]\n",
    "        else:\n",
    "            dots += self.pos_embedding\n",
    "            \n",
    "        if self.shifted:\n",
    "            dots[:,:, -nw_w:] += self.upper_lower_mask\n",
    "            dots[:,:, nw_w-1:: nw_w] += self.left_right_mask\n",
    "        \n",
    "        attn = dots.softmax(dim=-1)\n",
    "        \n",
    "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
    "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
    "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if self.shifted:\n",
    "            out = self.cyclic_back_shift(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d1d2722-25e6-4324-8ace-cd616cceabed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size,\n",
    "                relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,\n",
    "                                                                    heads=heads,\n",
    "                                                                    head_dim=head_dim,\n",
    "                                                                    shifted=shifted,\n",
    "                                                                    window_size=window_size,\n",
    "                                                                    relative_pos_embedding=relative_pos_embedding)))\n",
    "        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.attention_block(x)\n",
    "        x = self.mlp_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e40b2e8b-2339-4e2e-b423-2260ff6db9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging_Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.downscaling_factor = downscaling_factor\n",
    "        \n",
    "        self.patch_merge = nn.Conv2d(in_channels,\n",
    "                                    out_channels,\n",
    "                                    kernel_size=downscaling_factor,\n",
    "                                    stride=downscaling_factor,\n",
    "                                    padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.patch_merge(x).permute(0, 2, 3, 1)\n",
    "        return x\n",
    "    \n",
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.downscaling_factor = downscaling_factor\n",
    "        \n",
    "        self.patch_merge = nn.Unfold(\n",
    "                                    kernel_size=downscaling_factor,\n",
    "                                    stride=downscaling_factor,\n",
    "                                    padding=0)\n",
    "        \n",
    "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b,c,h,w = x.shape\n",
    "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
    "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30aa6530-47f5-462e-af82-7f026cef5284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StageModule(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads,\n",
    "                head_dim, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        assert layers %2==0,'Stage layers need to be divisible by 2 for regular and shifted blocks'\n",
    "        # self.patch_partition = PatchMerging_Conv(in_channels=in_channels, out_channels=hidden_dimension,\n",
    "        #                                         downscaling_factor=downscaling_factor)\n",
    "        \n",
    "        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n",
    "                                        downscaling_factor=downscaling_factor)\n",
    "        \n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(layers // 2):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, \n",
    "                         mlp_dim=hidden_dimension*4, shifted=False, window_size=window_size,\n",
    "                         relative_pos_embedding=relative_pos_embedding),\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, \n",
    "                         mlp_dim=hidden_dimension*4, shifted=True, window_size=window_size,\n",
    "                         relative_pos_embedding=relative_pos_embedding)\n",
    "            ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print('before patching merge: ', x.size()) (1,(3,96,192,384),(224,56,28,14),(224,56,28,14))\n",
    "        x = self.patch_partition(x)\n",
    "        # print('after patching merge: ', x.size()) (1, (56,28,14,7), (56,28,14,7),(96,192,384,768))\n",
    "        for regular_block, shifted_block in self.layers:\n",
    "            x=regular_block(x) #(1, (56,28,14,7), (56,28,14,7),(96,192,384,768))\n",
    "            \n",
    "            x=shifted_block(x) #(1, (56,28,14,7), (56,28,14,7),(96,192,384,768))\n",
    "        return x.permute(0,3,1,2) #(1,768,7,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a5070af-b095-4fcf-b9cc-206ed9731d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, *, hidden_dim, layers, heads, channels=3, num_classes=1000, head_dim=32,\n",
    "            window_size=7, downscaling_factors=(4,2,2,2), relative_pos_embedding=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stage1 = StageModule(in_channels=channels,hidden_dimension=hidden_dim,layers=layers[0],\n",
    "                                 downscaling_factor=downscaling_factors[0], num_heads=heads[0],\n",
    "                                 head_dim=head_dim, window_size=window_size, \n",
    "                                 relative_pos_embedding=relative_pos_embedding)\n",
    "        \n",
    "        self.stage2 = StageModule(in_channels=hidden_dim,hidden_dimension=hidden_dim*2, layers=layers[1],\n",
    "                                 downscaling_factor=downscaling_factors[1], num_heads=heads[1],\n",
    "                                 head_dim=head_dim, window_size=window_size, \n",
    "                                 relative_pos_embedding=relative_pos_embedding)\n",
    "        \n",
    "        self.stage3 = StageModule(in_channels=hidden_dim*2, hidden_dimension=hidden_dim*4, layers=layers[2],\n",
    "                                 downscaling_factor=downscaling_factors[2], num_heads=heads[2],\n",
    "                                 head_dim=head_dim, window_size=window_size, \n",
    "                                 relative_pos_embedding=relative_pos_embedding)\n",
    "        \n",
    "        self.stage4 = StageModule(in_channels=hidden_dim*4, hidden_dimension=hidden_dim*8, layers=layers[3],\n",
    "                                 downscaling_factor=downscaling_factors[3], num_heads=heads[3],\n",
    "                                 head_dim=head_dim, window_size=window_size, \n",
    "                                 relative_pos_embedding=relative_pos_embedding)\n",
    "        \n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim*8),\n",
    "            nn.Linear(hidden_dim*8, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, img):\n",
    "        x = self.stage1(img)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = x.mean(dim=[2,3])\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0be19492-f792-4527-aa61-de5ac640c91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def swin_t(hidden_dim=96, layers=(2,2,6,2), heads=(3,6,12,24), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9aa2c616-c634-4f7c-9af0-c1ffef56cbee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = swin_t(\n",
    "    hidden_dim=96,\n",
    "    layers=(2,2,6,2),\n",
    "    heads=(3,6,12,24),\n",
    "    channels=3,\n",
    "    num_classes=3,\n",
    "    head_dim=32,\n",
    "    window_size=7,\n",
    "    downscaling_factors=(4,2,2,2),\n",
    "    relative_pos_embedding=True\n",
    ")\n",
    "\n",
    "dummy_x = torch.randn(1,3,224,224)\n",
    "logits = net(dummy_x) # (1, 3)\n",
    "# print(\"network: \", net)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e76a4-bdb5-4587-bd5d-29dabb16ef2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d3f515-7dca-4bc9-bcd5-93d980d8e473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a3b0a-b72b-44e7-ab1d-69921ac99f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c251b90-ae96-4f2e-8d08-f5c3818ba8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2b012-0d4a-4be0-9df7-1acb605dc81a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c95eab-ada2-4a56-837e-8d992a3cddcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d78f97-02d2-4c7a-897e-40773b23869c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
