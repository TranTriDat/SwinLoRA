{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6267fe-530e-4b26-b1ca-860193f9fc37",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9338da0a-f5a1-4573-8cf3-8e21e86188a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "426b8012-dba4-4d03-a8f4-cfdf43e7fb43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import torch\n",
    "from torch import  nn, optim\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import swin_v2_t, swin_v2_b, efficientnet_b4\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.cuda.amp.autocast_mode import autocast\n",
    "from torch.cuda.amp.grad_scaler import GradScaler\n",
    "\n",
    "from torcheval.metrics.aggregation.auc import AUC\n",
    "from torcheval.metrics.functional.aggregation.auc import auc\n",
    "from torcheval.metrics import MulticlassAUROC, MulticlassAccuracy, Mean\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import loralib\n",
    "\n",
    "from utils import save_model, save_plots, SaveBestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3c773b-6ac8-4182-a016-d118e84fdcd3",
   "metadata": {},
   "source": [
    "# Working folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b060469-6005-4fea-9500-8ae5654b8c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/Dat/Vit/lora/Swin_From_Scratch/../ISIC2019_train/Image_DullRazor\n",
      "/home/user/Dat/Vit/lora/Swin_From_Scratch/../ISIC2019_valDR\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Concatenate the current directory with additional directories\n",
    "# train_dir = os.path.join(current_directory, \"../ISIC2019_train/Image\")\n",
    "train_dir = os.path.join(current_directory, \"../ISIC2019_train/Image_DullRazor\")\n",
    "\n",
    "val_dir = os.path.join(current_directory, \"../ISIC2019_valDR\")\n",
    "print(train_dir)\n",
    "print(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453340be-d9b4-4a36-a15c-7da142e1adf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_WORKERS = os.cpu_count()\n",
    "total_images = 0\n",
    "for i in os.listdir(train_dir):\n",
    "    tmp = os.path.join(train_dir, i)\n",
    "    for j in os.listdir(tmp):\n",
    "        total_images += 1\n",
    "print(total_images)\n",
    "NUM_WORKERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796249b9-8e3d-4b09-aa55-be83dab7375a",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d79c028-9eda-4d38-9055-0d9f12701ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMG_SIZE = 256\n",
    "IMG_SIZE = 32\n",
    "\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    val_dir: str,\n",
    "    # transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "#         transforms.CenterCrop((IMG_SIZE, IMG_SIZE)),\n",
    "        \n",
    "#         transforms.RandomHorizontalFlip(p=0.5),\n",
    "#         transforms.RandomRotation(35),\n",
    "        # transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "        #                      std=[0.229, 0.224, 0.225])\n",
    "        ])    \n",
    "    \n",
    "    valid_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        \n",
    "        # transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "        \n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "        #                      std=[0.229, 0.224, 0.225])\n",
    "        ]) \n",
    "\n",
    "    # Use ImageFolder to create dataset(s)\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "    val_data = datasets.ImageFolder(val_dir, transform=valid_transform)\n",
    "    \n",
    "    # Get class names\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    # Turn images into data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "      val_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c464ef-2b6e-47ad-b70a-ec8e5d88f471",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f7b081e7070>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f7a0adac8e0>,\n",
       " ['AK', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'SCC', 'VASC'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader, val_dataloader, class_names = create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    val_dir=val_dir,\n",
    "    # transform=manual_transforms, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "train_dataloader, val_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255d8c1-4f7b-4911-8052-2666e446ed1f",
   "metadata": {},
   "source": [
    "## class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "382549c0-ecba-498f-a2e6-534a5fd630bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AK', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'SCC', 'VASC']\n",
      "tensor([0.8210, 0.4920, 0.9750, 0.8690, 0.9660, 0.9900, 0.9910, 0.8960])\n"
     ]
    }
   ],
   "source": [
    "# Calculate class Weight\n",
    "class_weights = []\n",
    "for i in os.listdir(train_dir):\n",
    "    tmp = os.path.join(train_dir, i)\n",
    "    total_in_folder = 0\n",
    "    for j in os.listdir(tmp):\n",
    "        total_in_folder += 1\n",
    "    \n",
    "    tmp_weight = 1 - (total_in_folder / total_images)\n",
    "    class_weights.append(round(tmp_weight,3))\n",
    "class_weights = torch.Tensor(class_weights)\n",
    "print(class_names)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c6cf9a-1eab-4c2b-8a2a-57161c8d816d",
   "metadata": {},
   "source": [
    "# check gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1ab2533-bcc8-43ee-a372-57510299b24e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the GPU if available\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(len(class_names))\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb635d-6d1a-4c5b-8499-62df88d4e413",
   "metadata": {},
   "source": [
    "# Init mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9c1086a-722e-4e6b-a298-6c4541c0c233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Assuming IMG_SIZE is defined\n",
    "# IMG_SIZE = 224  # Define according to your needs\n",
    "# IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# Example using ResNet for similarity with EfficientNet's usage in TensorFlow\n",
    "\n",
    "# model = swin_v2_b(weights='IMAGENET1K_V1', progress=True)\n",
    "model = swin_v2_b(progress=True)\n",
    "\n",
    "# model = efficientnet_b4(weights='IMAGENET1K_V1', progress=True)\n",
    "\n",
    "\n",
    "# for param in model.parameters(): #freeze model\n",
    "#     # param.requires_grad = False\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "\n",
    "# model = nn.Sequential(*list(model.children())[:-2])  # Removing the fully connected layer & avgpool\n",
    "\n",
    "n_inputs = model.head.in_features\n",
    "print(n_inputs)\n",
    "\n",
    "model.head = nn.Sequential(\n",
    "    nn.Linear(n_inputs, 256),\n",
    "    # loralib.Linear(n_inputs,256,r=8),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    # nn.Linear(256, 512),\n",
    "    loralib.Linear(256,128,r=8),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    \n",
    "    nn.Linear(128, len(class_names)),\n",
    "    # loralib.Linear(128, len(class_names), r=64)\n",
    ")\n",
    "    \n",
    "# This sets requires_grad to False for all parameters\n",
    "# loralib.mark_only_lora_as_trainable(model)\n",
    "\n",
    "model = model.to(device)\n",
    "print(model.head)\n",
    "# print(model.classifier)\n",
    "\n",
    "# # Freeze all layers except the final classification layer\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"fc\" in name:  # Unfreeze the final classification layer\n",
    "#         param.requires_grad = True\n",
    "#     else:\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6310d094-a57b-4df9-92ea-b4baf7b06c59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CIFAR10Model(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), stride=1, padding=1)\n",
    "#         self.act1 = nn.ReLU()\n",
    "#         self.drop1 = nn.Dropout(0.3)\n",
    " \n",
    "#         self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=1, padding=1)\n",
    "#         self.act2 = nn.ReLU()\n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    " \n",
    "#         self.flat = nn.Flatten()\n",
    " \n",
    "#         self.fc3 = nn.Linear(8192, 512)\n",
    "#         self.act3 = nn.ReLU()\n",
    "#         self.drop3 = nn.Dropout(0.5)\n",
    " \n",
    "#         self.fc4 = nn.Linear(512, 8)\n",
    " \n",
    "#     def forward(self, x):\n",
    "#         # input 3x32x32, output 32x32x32\n",
    "#         x = self.act1(self.conv1(x))\n",
    "#         x = self.drop1(x)\n",
    "#         # input 32x32x32, output 32x32x32\n",
    "#         x = self.act2(self.conv2(x))\n",
    "#         # input 32x32x32, output 32x16x16\n",
    "#         x = self.pool2(x)\n",
    "#         # input 32x16x16, output 8192\n",
    "#         x = self.flat(x)\n",
    "#         # input 8192, output 512\n",
    "#         x = self.act3(self.fc3(x))\n",
    "#         x = self.drop3(x)\n",
    "#         # input 512, output 10\n",
    "#         x = self.fc4(x)\n",
    "#         return x\n",
    "    \n",
    "# model = CIFAR10Model()\n",
    "\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91098ed1-46f7-4b1f-a31a-4482b83ee853",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]             896\n",
      "              ReLU-2           [-1, 32, 32, 32]               0\n",
      "           Dropout-3           [-1, 32, 32, 32]               0\n",
      "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
      "              ReLU-5           [-1, 32, 32, 32]               0\n",
      "         MaxPool2d-6           [-1, 32, 16, 16]               0\n",
      "           Flatten-7                 [-1, 8192]               0\n",
      "            Linear-8                  [-1, 512]       4,194,816\n",
      "              ReLU-9                  [-1, 512]               0\n",
      "          Dropout-10                  [-1, 512]               0\n",
      "           Linear-11                    [-1, 8]           4,104\n",
      "================================================================\n",
      "Total params: 4,209,064\n",
      "Trainable params: 4,209,064\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.39\n",
      "Params size (MB): 16.06\n",
      "Estimated Total Size (MB): 17.45\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (3,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ab252-608f-40a1-8918-3134368f5ce8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Total param and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1113b2a-0074-44b6-ade3-a7ff175e0702",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87,205,248 total parameters.\n",
      "87,172,480 training parameters.\n"
     ]
    }
   ],
   "source": [
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "# loss_fn = nn.CrossEntropyLoss(\n",
    "#     # label_smoothing=0.11 ,\n",
    "#     # weight=class_weights\n",
    "# ).to(device)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.1, weight_decay=1e-8)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1 ,momentum=0.9)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1 ,momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.8)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "\n",
    "# save_best_model = SaveBestModel()\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "794c0c3d-493a-484e-bb1c-308cc117e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora rank 16 has 16520 training param 87,957,376\n",
    "# lora rank 32 has 33032 training param\n",
    "# lora rank 64 has 66056 training param\n",
    "# no lora has 8200 training param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08cf89a5-3f0a-40ca-ac22-a11f308c1089",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1689\n",
      "423\n"
     ]
    }
   ],
   "source": [
    "total=len(train_dataloader)\n",
    "print(total)\n",
    "print(len(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf354e-7354-4963-bae8-5a29571bdaf5",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2aa88c4c-8dbb-46f4-a0f2-05e49e759306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73d8c017-a2a9-44d5-bda3-10def54185e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "510df692-78ed-4b88-bff2-9e46460d6fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([8, 3, 32, 32])\n",
      "Labels batch shape: torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img.T, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3e2cd-f30f-46c8-a74f-e0f808b77f21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|███████████████| 20264/20264 [08:09<00:00, 41.36it/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: nan | train_acc: 0.0342 | test_loss: nan | test_acc: 0.0341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  98%|██████████████▋| 19880/20264 [05:52<00:07, 52.29it/s, loss=nan]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 4/10:  99%|██████████████▊| 20080/20264 [06:36<00:03, 47.72it/s, loss=nan]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 6/10: 100%|███████████████| 20264/20264 [08:08<00:00, 41.45it/s, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss: nan | train_acc: 0.0342 | test_loss: nan | test_acc: 0.0341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:  51%|███████▋       | 10328/20264 [03:09<03:11, 51.93it/s, loss=nan]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 9/10:  97%|██████████████▍| 19560/20264 [06:39<00:23, 29.66it/s, loss=nan]"
     ]
    }
   ],
   "source": [
    "scaler = GradScaler()\n",
    "# model = CIFAR10Model()\n",
    "# model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.1, weight_decay=1e-8)\n",
    "\n",
    " \n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    tq = tqdm(total=len(train_dataloader) * BATCH_SIZE, position=0, leave=True)\n",
    "    tq.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
    "    \n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "    # for inputs, labels in train_dataloader:\n",
    "        # forward, backward, and then weight update\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(enabled=True):\n",
    "            y_pred = model.forward(inputs)\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            train_loss += loss.item() \n",
    "            \n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        \n",
    "        # y_pred = model(inputs)\n",
    "        # loss = loss_fn(y_pred, labels)\n",
    "        \n",
    "        \n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        \n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == labels).sum().item()/len(y_pred)\n",
    "        \n",
    "        tq.update(BATCH_SIZE)\n",
    "        tq.set_postfix(loss='%.6f' % loss)\n",
    " \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    train_acc = train_acc / len(train_dataloader)\n",
    "    tq.close()\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    acc = 0\n",
    "    count = 0\n",
    "    val_loss = 0.0\n",
    "    for inputs, labels in val_dataloader:\n",
    "    # for inputs, labels in val_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        with autocast(enabled=True):\n",
    "            y_pred = model.forward(inputs)\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "        \n",
    "        # y_pred = model(inputs)\n",
    "        # loss = loss_fn(y_pred, labels)\n",
    "        \n",
    "        # Calculate and accumulate accuracy\n",
    "        # test_pred_labels = y_pred.argmax(dim=1)\n",
    "        # test_acc += ((test_pred_labels == labels).item()/len(test_pred_labels))\n",
    "        test_acc += (torch.argmax(y_pred, 1) == labels).float().sum()\n",
    "        count += len(labels)\n",
    "        \n",
    "    test_loss = test_loss / len(val_dataloader)\n",
    "    # test_acc = test_acc / len(val_dataloader)\n",
    "    \n",
    "    test_acc /= count\n",
    "    \n",
    "    \n",
    "    # acc /= count\n",
    "    # val_loss /= count\n",
    "    print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "    # print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))\n",
    "    # print(\"Epoch %d: model loss %.2f%%\" % (epoch, loss))\n",
    "    \n",
    " \n",
    "torch.save(model.state_dict(), \"cifar10model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dfa1a2-c5f8-4204-b513-5b120aa1c782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Loop for training\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "best_acc = - np.inf   # init to negative infinity\n",
    "best_weights = None\n",
    "train_loss_hist = []\n",
    "train_acc_hist = []\n",
    "val_loss_hist = []\n",
    "val_acc_hist = []\n",
    "\n",
    "# save_best_model = SaveBestModel()\n",
    "for epoch in range(num_epochs):\n",
    "    tq = tqdm(total=len(train_dataloader) * BATCH_SIZE, position=0, leave=True)\n",
    "    tq.set_description('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "    \n",
    "    train_epoch_loss = 0.0\n",
    "    train_epoch_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for X_train_batch, y_train_batch in train_dataloader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_train_pred = model(X_train_batch)\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch)\n",
    "        train_acc = multi_acc(y_train_pred, y_train_batch)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "        \n",
    "        tq.update(BATCH_SIZE)\n",
    "        tq.set_postfix(loss='%.6f' % train_loss)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_acc = 0\n",
    "        \n",
    "        tqe = tqdm(total=len(val_dataloader) * BATCH_SIZE, position=0, leave=True)\n",
    "        tqe.set_description('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        \n",
    "        model.eval()\n",
    "        for X_val_batch, y_val_batch in val_dataloader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            \n",
    "            y_val_pred = model(X_val_batch)\n",
    "                        \n",
    "            val_loss = criterion(y_val_pred, y_val_batch)\n",
    "            val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "            \n",
    "            val_epoch_loss += val_loss.item()\n",
    "            val_epoch_acc += val_acc.item()\n",
    "            \n",
    "            tqe.update(BATCH_SIZE)\n",
    "            tqe.set_postfix(loss='%.6f' % val_loss)\n",
    "\n",
    "    val_loss_hist.append(val_epoch_loss)\n",
    "    val_acc_hist.append(val_epoch_acc)\n",
    "    # train_losses.append(np.mean(train_epoch_loss))\n",
    "    # train_accuracies.append(np.mean(train_epoch_acc))\n",
    "    # val_losses.append(val_epoch_loss)\n",
    "    # val_accuracies.append(val_epoch_acc)\n",
    "    \n",
    "    # print(f\"Epoch [{epoch + 1}/{num_epochs}] Train Loss: {train_epoch_loss:.4f} Val Loss: {val_epoch_loss:.4f}\")\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Train Loss: {train_epoch_loss:.4f} Train Acc: {train_epoch_acc:.2f}% Val Loss: {val_epoch_loss:.4f} Val Acc: {val_epoch_acc:.2f}%\")\n",
    "\n",
    "save_model(num_epochs, model, optimizer, loss_fn, 'results', '27032024_15h')\n",
    "\n",
    "# # Plotting Training & Validation Loss\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(train_losses, '-x', label='Train')\n",
    "# plt.plot(val_losses, '-o', label='Val')\n",
    "# plt.title('Loss vs. No. of epochs')\n",
    "# plt.legend()\n",
    "\n",
    "# # Plotting Training & Validation Accuracy\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(train_accuracies, '-x', label='Train')\n",
    "# plt.plot(val_accuracies, '-o', label='Val')\n",
    "# plt.title('Accuracy vs. No. of epochs')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14173f73-f227-4219-a5c2-9be1b6479cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38206adf-b3af-451c-b39a-4ed66c3dc8b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# roc_auc = metrics.roc_auc_score(final_targets, final_outputs, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d97af-0268-42e6-bf0d-b1c761945226",
   "metadata": {},
   "source": [
    "## Performance on Valid/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c6ac635-75da-45db-8708-44b4ca6bb6a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 159/159 [01:15<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.11328202091967633\n",
      "5.0    2575\n",
      "4.0     904\n",
      "1.0     665\n",
      "2.0     525\n",
      "0.0     173\n",
      "6.0     126\n",
      "7.0      51\n",
      "3.0      48\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Performance on Valid/Test Data\n",
    "def overall_accuracy(model, test_loader, criterion):\n",
    "    \n",
    "    '''\n",
    "    Model testing \n",
    "    \n",
    "    Args:\n",
    "        model: model used during training and validation\n",
    "        test_loader: data loader object containing testing data\n",
    "        criterion: loss function used\n",
    "    \n",
    "    Returns:\n",
    "        test_loss: calculated loss during testing\n",
    "        accuracy: calculated accuracy during testing\n",
    "        y_proba: predicted class probabilities\n",
    "        y_truth: ground truth of testing data\n",
    "    '''\n",
    "    \n",
    "    y_proba = []\n",
    "    y_truth = []\n",
    "    test_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data in tqdm(test_loader):\n",
    "        # X, y = data[0].to('cpu'), data[1].to('cpu')\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        output = model(X)\n",
    "        test_loss += criterion(output, y.long()).item()\n",
    "        for index, i in enumerate(output):\n",
    "            y_proba.append(i[1])\n",
    "            y_truth.append(y[index])\n",
    "            if torch.argmax(i) == y[index]:\n",
    "                correct+=1\n",
    "            total+=1\n",
    "                \n",
    "    accuracy = correct/total\n",
    "    \n",
    "    y_proba_out = np.array([float(y_proba[i]) for i in range(len(y_proba))])\n",
    "    y_truth_out = np.array([float(y_truth[i]) for i in range(len(y_truth))])\n",
    "    \n",
    "    return test_loss, accuracy, y_proba_out, y_truth_out\n",
    "\n",
    "\n",
    "loss, acc, y_proba, y_truth = overall_accuracy(model, val_dataloader, \n",
    "                                               criterion = nn.CrossEntropyLoss())\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {acc}\")\n",
    "\n",
    "print(pd.value_counts(y_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32659948-7717-4497-a45d-dd250cbf0711",
   "metadata": {},
   "source": [
    "## Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4fd74246-6625-4d7b-bc3a-bee686c19440",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329.28403663635254"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea19e537-13bb-465a-98ff-894fc9fdc0d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve:\n",
    "\n",
    "def plot_ROCAUC_curve(y_truth, y_proba, fig_size):\n",
    "    \n",
    "    '''\n",
    "    Plots the Receiver Operating Characteristic Curve (ROC) and displays Area Under the Curve (AUC) score.\n",
    "    \n",
    "    Args:\n",
    "        y_truth: ground truth for testing data output\n",
    "        y_proba: class probabilties predicted from model\n",
    "        fig_size: size of the output pyplot figure\n",
    "    \n",
    "    Returns: void\n",
    "    '''\n",
    "    # fpr, tpr, threshold = roc_auc_score(y_truth, y_proba, multi_class='ovr')\n",
    "    fpr, tpr, threshold = roc_curve(y_truth, y_proba,pos_label=1)\n",
    "    auc_score = roc_auc_score(y_truth, y_proba, multi_class='ovr')\n",
    "    txt_box = \"AUC Score: \" + str(round(auc_score, 4))\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1],'--')\n",
    "    plt.annotate(txt_box, xy=(0.65, 0.05), xycoords='axes fraction')\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "    plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "#     plt.savefig('ROC.png')\n",
    "plot_ROCAUC_curve(y_truth, y_proba, (8, 8))\n",
    "# auc = roc_auc_score(y_truth, y_proba, multi_class='ovr')\n",
    "# print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ae3c3db5-a9b0-4d71-856c-580fd28ea240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_tpr_fpr(y_real, y_pred):\n",
    "    '''\n",
    "    Calculates the True Positive Rate (tpr) and the True Negative Rate (fpr) based on real and predicted observations\n",
    "    \n",
    "    Args:\n",
    "        y_real: The list or series with the real classes\n",
    "        y_pred: The list or series with the predicted classes\n",
    "        \n",
    "    Returns:\n",
    "        tpr: The True Positive Rate of the classifier\n",
    "        fpr: The False Positive Rate of the classifier\n",
    "    '''\n",
    "    \n",
    "    # Calculates the confusion matrix and recover each element\n",
    "    cm = confusion_matrix(y_real, y_pred)\n",
    "    TN = cm[0, 0]\n",
    "    FP = cm[0, 1]\n",
    "    FN = cm[1, 0]\n",
    "    TP = cm[1, 1]\n",
    "    \n",
    "    # Calculates tpr and fpr\n",
    "    tpr =  TP/(TP + FN) # sensitivity - true positive rate\n",
    "    fpr = 1 - TN/(TN+FP) # 1-specificity - false positive rate\n",
    "    \n",
    "    return tpr, fpr\n",
    "\n",
    "# Tpr, Fpr = calculate_tpr_fpr(y_truth, y_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "112c4c32-47de-48e7-9e56-56234b1d961d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SwinTransformer' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(val_dataloader))\n\u001b[0;32m----> 2\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(x)\n\u001b[1;32m      4\u001b[0m predict_label1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(prediction, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m true_label1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/skin_cancer/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SwinTransformer' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(val_dataloader))\n",
    "prediction = model.predict(x)\n",
    "\n",
    "predict_label1 = np.argmax(prediction, axis=-1)\n",
    "true_label1 = np.argmax(y, axis=-1)\n",
    "\n",
    "y = np.array(true_label1)\n",
    "\n",
    "# scores = np.array(predict_label1)\n",
    "# fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=9)\n",
    "# roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# lw = 2\n",
    "# plt.plot(fpr, tpr, color='darkorange',\n",
    "#  lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver operating characteristic (ROC)')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
